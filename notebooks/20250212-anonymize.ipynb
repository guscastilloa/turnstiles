{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "project_root = (os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Add the project root to sys.path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "import src.data.anonymizer as anonymizer\n",
    "from src.data.id_mapper import IDMapper\n",
    "from src.data.file_encoding import detect_file_encoding, batch_detect_encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre `salt` dans la fonction `create_anonymous_id` joue un rôle crucial dans la génération d'ID anonymes de manière sécurisée et déterministe. Voici une explication détaillée de son rôle :\n",
    "\n",
    "1. **Sécurité accrue** : Le `salt` est une valeur aléatoire ajoutée à l'identifiant (`identifier`) avant de générer le hachage. Cela empêche les attaques par dictionnaire et les attaques par force brute, car même si deux identifiants sont identiques, leurs hachages seront différents si des `salt` différents sont utilisés.\n",
    "\n",
    "2. **Déterminisme** : Si un `salt` spécifique est fourni, la fonction générera toujours le même ID anonyme pour un identifiant donné. Cela est utile pour garantir que les mêmes identifiants d'origine produisent les mêmes ID anonymes à chaque exécution, tant que le même `salt` est utilisé.\n",
    "\n",
    "3. **Génération aléatoire** : Si aucun `salt` n'est fourni, la fonction en génère un aléatoirement en utilisant `os.urandom(32)`. Cela garantit que chaque appel à la fonction avec le même identifiant produira un ID anonyme différent, ce qui peut être utile pour des besoins de sécurité spécifiques où le déterminisme n'est pas nécessaire.\n",
    "\n",
    "Example d'usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    hashed = anonymizer.create_anonymous_id(identifier='123123', salt=None)\n",
    "    print(hashed)\n",
    "\n",
    "# Whereas if we include a salt value, the hashed value will be different but consistent\n",
    "print(\"\\nWith salt:\")\n",
    "for i in range(3):\n",
    "    hashed = anonymizer.create_anonymous_id(identifier='123123', salt='000')\n",
    "    print(hashed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDMapper class\n",
    "It's easy to just create an unique identifer, and it will always return the same value once the salt is supplied (which it is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saltpath = Path().cwd().parent / 'config' / 'secure' / 'salt.key'\n",
    "\n",
    "mapper = IDMapper(\n",
    "    salt_path= saltpath)\n",
    "[mapper.create_anonymous_id(original_id=\"123123\") for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build sample datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_raw = os.path.join(project_root, 'data/raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MjÁlvarez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print full paths of all files in the raw data directory\n",
    "path = \"\".join([path_raw, '/MjÁlvarez'])\n",
    "files = os.listdir(path)\n",
    "\n",
    "# Read each file with pandas\n",
    "df_list = []\n",
    "for file in files:\n",
    "    file_path = os.path.join(path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_list.append(df)\n",
    "    print(f\"File: {os.path.basename(file_path)}\")\n",
    "    print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given that the dataframes in df_list are adjacency matrices, suggest way to anonimize the rows and columns of the adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymize and Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_anon = []\n",
    "\n",
    "for df in df_list:\n",
    "    # Récupérer les codes étudiants (première colonne et en-têtes)\n",
    "    student_ids = set([df['CARNET'].values[0]] + list(df.columns[1:]))\n",
    "\n",
    "    # Create a dictionary maapping for IDs\n",
    "    id_mapping = {}\n",
    "    for student_id in student_ids:\n",
    "        anonymous_id = mapper.add_identifier(str(student_id), source='survey')\n",
    "        id_mapping[str(student_id)] = anonymous_id\n",
    "\n",
    "    # Rename columns and the first column\n",
    "    df_anon = df.copy()\n",
    "    df_anon.columns = ['CARNET'] + [id_mapping[str(col)] for col in df.columns[1:]]\n",
    "    df_anon['CARNET'] = df_anon['CARNET'].map(lambda x: id_mapping[str(x)])\n",
    "    \n",
    "    df_list_anon.append(df_anon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_anon[1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder la matrice anonymisée\n",
    "df_anon.to_csv(\"Ciencia_Politica_Amistad_20182_anonymized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list_anon[0].to_csv(\"../tests/data/Ciencia_Politica_Trabajos_20182_sample_anon.csv\", index = False)\n",
    "\n",
    "# Save mapping\n",
    "# output_dir = Path('../data/intermediate')\n",
    "# mapper.save_mappings(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrustExperiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revise the encoding of the files TrustExperiment\n",
    "\n",
    "encodings = batch_detect_encodings('../data/raw/TrustExperiment', pattern='*.csv')\n",
    "for filename, encoding in encodings.items():\n",
    "    print(f\"{filename}: {encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\".join([path_raw, '/TrustExperiment'])\n",
    "files = os.listdir(path)\n",
    "\n",
    "\n",
    "# Read files with the encoding in the encodings dictionary\n",
    "df_list = []\n",
    "for file in files:\n",
    "    file_path = os.path.join(path, file)\n",
    "    df = pd.read_csv(file_path, encoding=encodings[file])\n",
    "    df_list.append(df)\n",
    "    print(f\"File: {os.path.basename(file_path)}\")\n",
    "    print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got to revise the current `MasterIDsFile.csv` and anonymize all the CSV for all the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_list[0].anonymousID.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce sample data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list[4].head(20).to_csv('../tests/data/TestExperiment_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turnstile "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymize Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "# from id_mapper import IDMapper\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "def anonymize_turnstile_files(input_dir: Path, output_dir: Path, salt_path: Path):\n",
    "    \"\"\"\n",
    "    Anonymize all turnstile CSV files in the input directory\n",
    "    \n",
    "    Args:\n",
    "        input_dir: Directory containing the original CSV files\n",
    "        output_dir: Directory where anonymized files will be saved\n",
    "        salt_path: Path to the salt file for consistent anonymization\n",
    "    \"\"\"\n",
    "    # Initialize the IDMapper\n",
    "    mapper = IDMapper(salt_path=salt_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process each CSV file\n",
    "    for csv_file in glob.glob(str(input_dir / \"P2000*.csv\")):\n",
    "        file_path = Path(csv_file)\n",
    "        \n",
    "        # Read the CSV file\n",
    "        print(f\"Reading {file_path}\")\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=',')\n",
    "        except pd.errors.ParserError:\n",
    "            df = pd.read_csv(file_path, delimiter=';')\n",
    "        \n",
    "        # Anonymize student IDs\n",
    "        assert 'carnet' in df.columns, \"Column 'carnet' does not exist in the dataframe\"\n",
    "        df['carnet'] = df['carnet'].astype(str).apply(\n",
    "            lambda x: mapper.add_identifier(x, source='turnstile')\n",
    "        )\n",
    "        \n",
    "        # Save anonymized file\n",
    "        output_file = output_dir / f\"anon_{file_path.name}\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        logging.info(f\"Processed {file_path.name}\")\n",
    "    \n",
    "    # Save the mapping files\n",
    "    mapper.save_mappings(output_dir / \"mappings\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_DIR = Path(\"data/intermediate/daily\")\n",
    "OUTPUT_DIR = Path(\"data/intermediate/\")\n",
    "SALT_PATH = Path(\"config/secure/salt.key\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Run anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anonymize_turnstile_files(Path(\"../tests/data\"), OUTPUT_DIR, SALT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
